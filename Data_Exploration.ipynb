{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9917a9ec-c60f-4178-9ef9-d3bc3f0d8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8dd6579-37ca-4ff6-93fc-505799020bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"aaronjpi/fruits-vegetables-price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7e25a2-61c6-4c9a-85ef-189e6e70d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Convert price columns from strings to numeric values (removing '$')\n",
    "# 2) Convert averagespread from string percentages to decimal values\n",
    "# 3) Check for missing values (none visible in this sample)\n",
    "# 4) Split the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62ddd0af-7183-4484-a1b6-7029f0803529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nulls:  productname         0\n",
      "date                0\n",
      "farmprice           0\n",
      "atlantaretail       0\n",
      "chicagoretail       0\n",
      "losangelesretail    0\n",
      "newyorkretail       0\n",
      "averagespread       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning/processing\n",
    "df = pd.DataFrame(ds['train'])\n",
    "\n",
    "null_count = df.isnull().sum()\n",
    "print(\"Number of nulls: \", null_count)\n",
    "\n",
    "# Filter for only strawberry data\n",
    "strawberry_data = df[df['productname'] == 'Strawberries'].copy()\n",
    "\n",
    "price_columns = ['farmprice', 'atlantaretail', 'chicagoretail', 'losangelesretail', 'newyorkretail']\n",
    "\n",
    "# Remove the dollar sign from price values\n",
    "for column in price_columns:\n",
    "    strawberry_data[column] = strawberry_data[column].str.replace('$', '').str.strip()\n",
    "\n",
    "# Convert averagespread from percentage to float\n",
    "strawberry_data['averagespread'] = (strawberry_data['averagespread'].str.rstrip('%').str.replace(',', '').astype(float) / 100)\n",
    "\n",
    "# Sort data by date (oldest to newest)\n",
    "strawberry_data = strawberry_data.sort_values('date')\n",
    "\n",
    "#Create extra features for the model\n",
    "# This adds them right to the data\n",
    "strawberry_data['month'] = strawberry_data['date'].dt.month\n",
    "strawberry_data['day_of_week'] = strawberry_data['date'].dt.dayofweek\n",
    "strawberry_data['is_holiday_season'] = ((strawberry_data['month'] >= 11) | (strawberry_data['month'] <= 1)).astype(int)\n",
    "\n",
    "# Create lag features (previous weeks' prices)\n",
    "# By including multiple lag periods (1, 2, and 4 weeks) \n",
    "# it gives the model with different time periods that help it see\n",
    "# long term and short term trends\n",
    "for lag in [1, 2, 4]:\n",
    "    for column in price_columns:\n",
    "        strawberry_data[f'{column}_lag_{lag}'] = strawberry_data[column].shift(lag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af75bb34-826d-47ea-af03-d7a0b32c5c49",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_1647208/2708881014.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     18\u001b[39m \n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Scale the features\u001b[39;00m\n\u001b[32m     20\u001b[39m scaler_perceptron = StandardScaler()\n\u001b[32m     21\u001b[39m X_train_perceptron = scaler_perceptron.fit_transform(X_train_perceptron)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m X_test_perceptron = scaler_perceptron.transform(X_test_perceptron)\n",
      "\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/sklearn/utils/_set_output.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m     @wraps(f)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m wrapped(self, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         data_to_wrap = f(self, X, *args, **kwargs)\n\u001b[32m    317\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m isinstance(data_to_wrap, tuple):\n\u001b[32m    318\u001b[39m             \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m             return_tuple = (\n",
      "\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/sklearn/preprocessing/_data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1090\u001b[39m         xp, _, X_device = get_namespace_and_device(X)\n\u001b[32m   1091\u001b[39m         check_is_fitted(self)\n\u001b[32m   1092\u001b[39m \n\u001b[32m   1093\u001b[39m         copy = copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m self.copy\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m         X = validate_data(\n\u001b[32m   1095\u001b[39m             self,\n\u001b[32m   1096\u001b[39m             X,\n\u001b[32m   1097\u001b[39m             reset=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/sklearn/utils/validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2898\u001b[39m             out = y\n\u001b[32m   2899\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2900\u001b[39m             out = X, y\n\u001b[32m   2901\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2902\u001b[39m         out = check_array(X, input_name=\u001b[33m\"X\"\u001b[39m, **check_params)\n\u001b[32m   2903\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2904\u001b[39m         out = _check_y(y, **check_params)\n\u001b[32m   2905\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/sklearn/utils/validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1019\u001b[39m                         )\n\u001b[32m   1020\u001b[39m                     array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1021\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1022\u001b[39m                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1024\u001b[39m                 raise ValueError(\n\u001b[32m   1025\u001b[39m                     \u001b[33m\"Complex data not supported\\n{}\\n\"\u001b[39m.format(array)\n\u001b[32m   1026\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m complex_warning\n",
      "\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/sklearn/utils/_array_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    874\u001b[39m         \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[32m    875\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    876\u001b[39m             array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    877\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m             array = numpy.asarray(array, order=order, dtype=dtype)\n\u001b[32m    879\u001b[39m \n\u001b[32m    880\u001b[39m         \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n",
      "\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m   2167\u001b[39m             )\n\u001b[32m   2168\u001b[39m         values = self._values\n\u001b[32m   2169\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2170\u001b[39m             \u001b[38;5;66;03m# Note: branch avoids `copy=None` for NumPy 1.x support\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2171\u001b[39m             arr = np.asarray(values, dtype=dtype)\n\u001b[32m   2172\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2173\u001b[39m             arr = np.array(values, dtype=dtype, copy=copy)\n\u001b[32m   2174\u001b[39m \n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: ''"
     ]
    }
   ],
   "source": [
    "# Preparing data for perceptron model\n",
    "\n",
    "# Create binary target: 1 if price increased from previous week, 0 if not\n",
    "strawberry_data['price_increased'] = (strawberry_data['farmprice'] > strawberry_data['farmprice'].shift(1)).astype(int)\n",
    "\n",
    "# Prepare features and target\n",
    "X_perceptron = X_perceptron.apply(pd.to_numeric, errors='coerce')\n",
    "X_perceptron = X_perceptron.dropna()\n",
    "y_perceptron = y_perceptron.iloc[X_perceptron.index]\n",
    "\n",
    "X_perceptron = strawberry_data.drop(['productname', 'date', 'price_increased', 'averagespread'], axis=1)\n",
    "y_perceptron = strawberry_data['price_increased'].iloc[len(strawberry_data) - len(X_perceptron):]\n",
    "\n",
    "# Calculate the split point (use the last 20% of data as test set)\n",
    "train_size = int(len(X_perceptron) * 0.8)\n",
    "\n",
    "# Split the data chronologically\n",
    "X_train_perceptron = X_perceptron.iloc[:train_size]\n",
    "X_test_perceptron = X_perceptron.iloc[train_size:]\n",
    "y_train_perceptron = y_perceptron.iloc[:train_size]\n",
    "y_test_perceptron = y_perceptron.iloc[train_size:]\n",
    "\n",
    "# Scale the features\n",
    "scaler_perceptron = StandardScaler()\n",
    "X_train_perceptron = scaler_perceptron.fit_transform(X_train_perceptron)\n",
    "X_test_perceptron = scaler_perceptron.transform(X_test_perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54f2c9f-8e24-480f-bad6-0ccac9a9534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for LSTM model\n",
    "\n",
    "# Select features for LSTM\n",
    "features_lstm = ['farmprice', 'atlantaretail', 'chicagoretail', 'losangelesretail', 'newyorkretail', \n",
    "                'month', 'day_of_week', 'is_holiday_season']\n",
    "\n",
    "# Scale the data\n",
    "scaler_lstm = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler_lstm.fit_transform(data_lstm)\n",
    "\n",
    "# Create sequences (time windows)\n",
    "# These are needed for LSTM so it can see change in price over time\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        # For prediction, use farm price\n",
    "        y.append(data[i + seq_length, 0]) \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_length = 4  # 4 weeks for the length of the time windows\n",
    "X_lstm, y_lstm = create_sequences(scaled_data, seq_length)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(len(X_lstm) * 0.8)\n",
    "X_train_lstm = X_lstm[:train_size]\n",
    "X_test_lstm = X_lstm[train_size:]\n",
    "y_train_lstm = y_lstm[:train_size]\n",
    "y_test_lstm = y_lstm[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff3539-6771-4492-aa7d-50e59b26e30a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
